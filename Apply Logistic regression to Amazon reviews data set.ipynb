{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc regression on Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing library\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('database.sqlite') \n",
    "\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "#creating new datasets after applying filter on reviews dataset\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", con) \n",
    "\n",
    "# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "# with the help of this method returning positive and negative based on the score\n",
    "\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filtered_data.shape) #looking at the size of the data\n",
    "filtered_data.head() # top five reviews, just for understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the reviews which is same on the basis of few features \n",
    "final=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape # after deleting, look at shape again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head() # look at top five reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we know that helfulnessnumerator will not  be greater than helpfullness denominator \n",
    "# So we will remove that reviews because that reviews no make sense \n",
    "\n",
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after removing reviews in above cell which were useless, no make sense\n",
    "# so look at the reviewsprint(final.shape)\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop-words\n",
    "\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence):  #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence):  #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "# this code takes a while to run as it needs to run on 500k sentences.\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    \n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)): # assure that cleaned words are alphabetical and length is greater than 2\n",
    "                if(cleaned_words.lower() not in stop):  # thos words who were not in stop words\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8') # changing cleaned words into lower case\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive':  #IF words are positive \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative': # if words are negative\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    \n",
    "    \n",
    "    final_string.append(str1) #final_string dataset appending string after cleaning words\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought sever vital can dog food product found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arriv label jumbo salt peanut peanut a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confect around centuri light pillowi citrus ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>look secret ingredi robitussin believ found go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffi great price wide assort yummi taff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  bought sever vital can dog food product found ...  \n",
       "1  product arriv label jumbo salt peanut peanut a...  \n",
       "2  confect around centuri light pillowi citrus ge...  \n",
       "3  look secret ingredi robitussin believ found go...  \n",
       "4  great taffi great price wide assort yummi taff...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape # look at the shape of final dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting data on the basis of time stamp for time based splitting \n",
    "sorted_data=final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "final=sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DVS0283\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing library\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_tr, X_tes, y_tr, y_test = cross_validation.train_test_split(final['CleanedText'].values,final['Score'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (254919, 12703)\n",
      "the number of unique words  12703\n"
     ]
    }
   ],
   "source": [
    "# we are collecting all the split words in the form of tokens matrix\n",
    "count_vect = CountVectorizer(min_df=10) #in scikit-learn\n",
    "X_train = count_vect.fit_transform(X_tr)#giving training data set to vectorize the training data\n",
    "X_test = count_vect.transform(X_tes) #giving testing data set to vectorize the testing data\n",
    "print(\"the type of count vectorizer \",type(X_train))\n",
    "print(\"the shape of out text BOW vectorizer \",X_train.get_shape())\n",
    "print(\"the number of unique words \", X_train.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data-preprocessing: Standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_vec = sc.fit_transform(X_train)\n",
    "X_test_vec = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.9451678769403882\n",
      "0.9195255006773332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', cv=5,n_jobs=-1)\n",
    "model.fit(X_train_vec, y_tr)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model with the optimal lambda value \n",
    "lr1 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr1.fit(X_train_vec,y_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to check multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7939848"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape1=X_train_vec.nnz # number of non zero element available in sparse matrix\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0,      0,      0, ..., 254918, 254918, 254918], dtype=int32),\n",
       " array([ 1477,  3981,  4678, ..., 11321, 11397, 11645], dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=X_train_vec.nonzero()  # function to get the row and column indices of non zero element in sparse matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254919, 12703)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sparse module from SciPy package \n",
    "from scipy import sparse\n",
    "\n",
    "import sys\n",
    "data=[] #initializing data to put it in sparse matrix \n",
    "row_ind=a[0]  # row indices of non zero element \n",
    "col_ind=a[1]# # column indices of non zero element \n",
    "\n",
    "# data to be stored in Csr sparse matrix\n",
    "data[0:shape1]=[0.0001 for i in range(0,shape1)]\n",
    "\n",
    "# create csr sparse matrix with the help of row,column indices and data \n",
    "X_train_vec_csr = sparse.csr_matrix((data, (row_ind, col_ind)))\n",
    "\n",
    "# shape of new csr matrix\n",
    "X_train_vec_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new sparse matrix after adding epsilon value in our sparse feature matrix\n",
    "X_train_vec_csr1=X_train_vec_csr+X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model with the optimal lambda value with new sparse matrix features\n",
    "lr2 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr2.fit(X_train_vec_csr1,y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=np.absolute(lr2.coef_-lr1.coef_) #absolute diff of coefficient of both the matrix to check multicollinearity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2=np.sort(arr1.ravel())[::-1] #sorting the array for better understanding of difference in coefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.91293309e-06, 1.78844256e-06, 1.71262469e-06, 1.51676013e-06,\n",
       "       1.30210137e-06, 1.29873701e-06, 1.12637662e-06, 1.09039557e-06,\n",
       "       1.08716796e-06, 1.07800618e-06])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2[0:10]  # here is our top difference of coefficient of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there is little difference between our features sparse matrix coefficient and our second sparse matrix after adding epsilon in coeeficient so we can say there is no collinearity exist in our features so we can find important features as usual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12703)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3=lr1.coef_ #to find weight of most important features\n",
    "arr2=np.absolute(lr1.coef_) # absolute value for sorting in decreasing order\n",
    "arr2.shape # shape of weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=np.argsort(arr2.ravel())[::-1] #sorting array to find index of most imp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "0.3592922625017345\n",
      "love\n",
      "0.2958555610940963\n",
      "best\n",
      "0.24476749531484415\n",
      "good\n",
      "0.2199514891031782\n",
      "disappoint\n",
      "-0.2078621695739689\n"
     ]
    }
   ],
   "source": [
    "list2=count_vect.get_feature_names()\n",
    "for i in list1[0:5]: \n",
    "    print(list2[i])  # most important features\n",
    "    print(arr3[0,i]) # weight of most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.9451678769403882\n",
      "0.9195255006773332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(),param_dist,scoring='accuracy',cv=5,n_jobs=-1,n_iter=5)\n",
    "                               \n",
    "random_search.fit(X_train_vec, y_tr)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.948983793651004\n",
      "0.9216307252956467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_vec, y_tr)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model with the optimal lambda value \n",
    "lr1 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr1.fit(X_train_vec,y_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to check multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7939848"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape1=X_train_vec.nnz # number of non zero element available in sparse matrix\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0,      0,      0, ..., 254918, 254918, 254918], dtype=int32),\n",
       " array([ 1477,  3981,  4678, ..., 11321, 11397, 11645], dtype=int32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=X_train_vec.nonzero() # function to get the row and column indices of non zero element in sparse matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254919, 12703)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sparse module from SciPy package \n",
    "from scipy import sparse\n",
    "\n",
    "import sys\n",
    "data=[] #initializing data to put it in sparse matrix \n",
    "row_ind=a[0]  # row indices of non zero element \n",
    "col_ind=a[1]# # column indices of non zero element \n",
    "\n",
    "# data to be stored in Csr sparse matrix\n",
    "data[0:shape1]=[0.0001 for i in range(0,shape1)]\n",
    "\n",
    "# create csr sparse matrix with the help of row,column indices and data \n",
    "X_train_vec_csr = sparse.csr_matrix((data, (row_ind, col_ind)))\n",
    "\n",
    "# shape of new csr matrix\n",
    "X_train_vec_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new sparse matrix after adding epsilon value in our sparse feature matrix\n",
    "X_train_vec_csr1=X_train_vec_csr+X_train_vec\n",
    "\n",
    "# training the model with the optimal lambda value with new sparse matrix features\n",
    "lr2 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr2.fit(X_train_vec_csr1,y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=np.absolute(lr2.coef_-lr1.coef_) #absolute diff of coefficient of both the matrix to check multicollinearity features\n",
    "arr2=np.sort(arr1.ravel())[::-1] #sorting the array for better understanding of difference in coefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00114352, 0.00080803, 0.00061883, 0.00052609, 0.00050936,\n",
       "       0.00040204, 0.00036598, 0.00031517, 0.00031244, 0.0003102 ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2[0:10] # here is our top difference of coefficient of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there is little difference between our features sparse matrix coefficient and our second sparse matrix after adding epsilon in coeeficient so we can say there is no collinearity exist in our features so we can find important features as usual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12703)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3=lr1.coef_ #to find weight of most important features\n",
    "arr2=np.absolute(lr1.coef_) # absolute value for sorting in decreasing order\n",
    "arr2.shape # shape of weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=np.argsort(arr2.ravel())[::-1] #sorting array to find index of most imp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "0.8023428526870029\n",
      "best\n",
      "0.5904135352417754\n",
      "love\n",
      "0.5830848186945969\n",
      "delici\n",
      "0.5299930076366527\n",
      "perfect\n",
      "0.4759886008911331\n"
     ]
    }
   ],
   "source": [
    "list2=count_vect.get_feature_names()\n",
    "for i in list1[0:5]: \n",
    "    print(list2[i])  # most important features\n",
    "    print(arr3[0,i]) # weight of most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9489818323244447\n",
      "0.9216307252956467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(penalty='l1'),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                   \n",
    "random_search.fit(X_train_vec, y_tr)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l1 regularization with different value of C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11643\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty='l1');\n",
    "clf.fit(X_train_vec, y_tr);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5937\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1');\n",
    "clf.fit(X_train_vec, y_tr);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty='l1');\n",
    "clf.fit(X_train_vec, y_tr);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0005, penalty='l1');\n",
    "clf.fit(X_train_vec, y_tr);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11641\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.10, penalty='l1');\n",
    "clf.fit(X_train_vec, y_tr);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if C decreases it's mean lambda increases and we can observe that if lambda increases then number of non zero elements decreases it's mean sparsity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_tr, X_tes, y_train, y_test = cross_validation.train_test_split(final['CleanedText'].values, final['Score'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "X_train_tfidf = tf_idf_vect.fit_transform(X_tr)\n",
    "X_test_tfidf=tf_idf_vect.transform(X_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-preprocessing: Standardizing the data\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_tfidf_vec = sc.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf_vec = sc.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.9999882315559058\n",
      "0.9214751217369018\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_tfidf_vec, y_train)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_tfidf_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model with the optimal lambda value \n",
    "lr1 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr1.fit(X_train_tfidf_vec,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  to check multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14483646"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape1=X_train_tfidf_vec.nnz # number of non zero element available in sparse matrix\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0,      0,      0, ..., 254918, 254918, 254918], dtype=int32),\n",
       " array([ 36487, 266050, 294819, ...,   5172,  82545, 181159], dtype=int32))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=X_train_tfidf_vec.nonzero() # function to get the row and column indices of non zero element in sparse matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254919, 305513)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sparse module from SciPy package \n",
    "from scipy import sparse\n",
    "\n",
    "import sys\n",
    "data=[] #initializing data to put it in sparse matrix \n",
    "row_ind=a[0]  # row indices of non zero element \n",
    "col_ind=a[1]# # column indices of non zero element \n",
    "\n",
    "# data to be stored in Csr sparse matrix\n",
    "data[0:shape1]=[0.0001 for i in range(0,shape1)]\n",
    "\n",
    "# create csr sparse matrix with the help of row,column indices and data \n",
    "X_train_tfidf_vec_csr = sparse.csr_matrix((data, (row_ind, col_ind)))\n",
    "\n",
    "# shape of new csr matrix\n",
    "X_train_tfidf_vec_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new sparse matrix after adding epsilon value in our sparse feature matrix\n",
    "X_train_tfidf_vec_csr1=X_train_tfidf_vec_csr+X_train_tfidf_vec\n",
    "\n",
    "# training the model with the optimal lambda value with new sparse matrix features\n",
    "lr2 = LogisticRegression(C=model.best_params_['C'])\n",
    "lr2.fit(X_train_tfidf_vec_csr1,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=np.absolute(lr2.coef_-lr1.coef_) #absolute diff of coefficient of both the matrix to check multicollinearity features\n",
    "arr2=np.sort(arr1.ravel())[::-1] #sorting the array for better understanding of difference in coefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.02476438e-05, 2.34309212e-05, 1.97582890e-05, 1.91828160e-05,\n",
       "       1.75390936e-05, 1.73465807e-05, 1.63124394e-05, 1.51056552e-05,\n",
       "       1.35875420e-05, 1.35798101e-05])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2[0:10] # here is our top difference of coefficient of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there is little difference between our features sparse matrix coefficient and our second sparse matrix after adding epsilon in coeeficient so we can say there is no collinearity exist in our features so we can find important features as usual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 305513)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3=lr1.coef_ #to find weight of most important features\n",
    "arr2=np.absolute(lr1.coef_) # absolute value for sorting in decreasing order\n",
    "arr2.shape # shape of weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=np.argsort(arr2.ravel())[::-1] #sorting array to find index of most imp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "0.1315125557118817\n",
      "love\n",
      "0.12583890714644988\n",
      "best\n",
      "0.09391876229981895\n",
      "good\n",
      "0.09364575512247429\n",
      "disappoint\n",
      "-0.08067444353364092\n"
     ]
    }
   ],
   "source": [
    "list2=tf_idf_vect.get_feature_names()\n",
    "for i in list1[0:5]: \n",
    "    print(list2[i])  # most important features\n",
    "    print(arr3[0,i]) # weight of most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.9999882315559058\n",
      "0.9214751217369018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                   \n",
    "random_search.fit(X_train_tfidf_vec, y_train)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_tfidf_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9999882315559058\n",
      "0.9296305788452386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_tfidf_vec, y_train)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_tfidf_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model with the optimal lambda value \n",
    "lr1 = LogisticRegression(penalty='l1',C=model.best_params_['C'])\n",
    "lr1.fit(X_train_tfidf_vec,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to check multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14483646"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape1=X_train_tfidf_vec.nnz # number of non zero element available in sparse matrix\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0,      0,      0, ..., 254918, 254918, 254918], dtype=int32),\n",
       " array([ 36487, 266050, 294819, ...,   5172,  82545, 181159], dtype=int32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=X_train_tfidf_vec.nonzero() # function to get the row and column indices of non zero element in sparse matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254919, 305513)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sparse module from SciPy package \n",
    "from scipy import sparse\n",
    "\n",
    "import sys\n",
    "data=[] #initializing data to put it in sparse matrix \n",
    "row_ind=a[0]  # row indices of non zero element \n",
    "col_ind=a[1]# # column indices of non zero element \n",
    "\n",
    "# data to be stored in Csr sparse matrix\n",
    "data[0:shape1]=[0.0001 for i in range(0,shape1)]\n",
    "\n",
    "# create csr sparse matrix with the help of row,column indices and data \n",
    "X_train_tfidf_vec_csr = sparse.csr_matrix((data, (row_ind, col_ind)))\n",
    "\n",
    "# shape of new csr matrix\n",
    "X_train_tfidf_vec_csr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new sparse matrix after adding epsilon value in our sparse feature matrix\n",
    "X_train_tfidf_vec_csr1=X_train_tfidf_vec_csr+X_train_tfidf_vec\n",
    "\n",
    "# training the model with the optimal lambda value with new sparse matrix features\n",
    "lr2 = LogisticRegression(penalty='l1',C=model.best_params_['C'])\n",
    "lr2.fit(X_train_tfidf_vec_csr1,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1=np.absolute(lr2.coef_-lr1.coef_) #absolute diff of coefficient of both the matrix to check multicollinearity features\n",
    "arr2=np.sort(arr1.ravel())[::-1] #sorting the array for better understanding of difference in coefficent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00494096, 0.00373867, 0.00373752, 0.00346236, 0.00270984,\n",
       "       0.0027069 , 0.0025327 , 0.00192536, 0.00191478, 0.00191142])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2[0:10] # here is our top difference of coefficient of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there is little difference between our features sparse matrix coefficient and our second sparse matrix after adding epsilon in coeeficient so we can say there is no collinearity exist in our features so we can find important features as usual "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 305513)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr3=lr1.coef_ #to find weight of most important features\n",
    "arr2=np.absolute(lr1.coef_) # absolute value for sorting in decreasing order\n",
    "arr2.shape # shape of weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=np.argsort(arr2.ravel())[::-1] #sorting array to find index of most imp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "0.8716541974128199\n",
      "love\n",
      "0.6327252052368632\n",
      "best\n",
      "0.6100860385058007\n",
      "delici\n",
      "0.5168643601019093\n",
      "perfect\n",
      "0.4581109688374052\n"
     ]
    }
   ],
   "source": [
    "list2=tf_idf_vect.get_feature_names()\n",
    "for i in list1[0:5]: \n",
    "    print(list2[i])  # most important features\n",
    "    print(arr3[0,i]) # weight of most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.9999882315559058\n",
      "0.9296580382967817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(penalty='l1'),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                   \n",
    "random_search.fit(X_train_tfidf_vec, y_train)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_tfidf_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l1 regularization with different value of C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48892\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty='l1');\n",
    "clf.fit(X_train_tfidf_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31972\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1');\n",
    "clf.fit(X_train_tfidf_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty='l1');\n",
    "clf.fit(X_train_tfidf_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0005, penalty='l1');\n",
    "clf.fit(X_train_tfidf_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98690\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty='l1');\n",
    "clf.fit(X_train_tfidf_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if C decreases it's mean lambda increases and we can observe that if lambda increases then number of non zero elements decreases it's mean sparsity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on avgw2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_tr, X_tes, y_train, y_test = cross_validation.train_test_split(final['CleanedText'].values, final['Score'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train data we are finding avg w2v for each train data\n",
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in X_tr:\n",
    "    list_of_sent.append(sent.split())\n",
    "\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_avgw2v = []; # the X_train_avgw2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    X_train_avgw2v.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train data we are finding avg w2v for each train data\n",
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in X_tes:\n",
    "    list_of_sent.append(sent.split())\n",
    "\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []; # the X_train_avgw2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    X_test.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-preprocessing: Standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_avgw2v_vec = sc.fit_transform(X_train_avgw2v)\n",
    "X_test_avgw2v_vec = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8973948603840225\n",
      "0.8456046571229817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_avgw2v_vec, y_train)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_avgw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8973948603840225\n",
      "0.8456046571229817\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                               \n",
    "random_search.fit(X_train_avgw2v_vec, y_train)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_avgw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8973497481304076\n",
      "0.8459433236920147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_avgw2v_vec, y_train)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_avgw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8973595551325287\n",
      "0.8459433236920147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(penalty='l1'),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                \n",
    "random_search.fit(X_train_avgw2v_vec, y_train)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_avgw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l1 regularization with different value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty='l1');\n",
    "clf.fit(X_train_avgw2v_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1');\n",
    "clf.fit(X_train_avgw2v_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty='l1');\n",
    "clf.fit(X_train_avgw2v_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0005, penalty='l1');\n",
    "clf.fit(X_train_avgw2v_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty='l1');\n",
    "clf.fit(X_train_avgw2v_vec, y_train);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if C decreases it's mean lambda increases and we can observe that if lambda increases then number of non zero elements decreases it's mean sparsity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression on tf-idfw2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final=final.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_1, X_tes, y_1, y_test = cross_validation.train_test_split(my_final['CleanedText'].values, my_final['Score'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train data \n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_tf_idf = tf_idf_vect.fit_transform(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own tfidf Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in X_1:\n",
    "    list_of_sent.append(sent.split())\n",
    "    \n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "X_train_tfidfw2v = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    X_train_tfidfw2v.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train data \n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_tf_idf = tf_idf_vect.fit_transform(X_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your own tfidf Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in X_tes:\n",
    "    list_of_sent.append(sent.split())\n",
    "w2v_model=Word2Vec(list_of_sent,min_count=5,size=50, workers=4)\n",
    "w2v_words = list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "X_test = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    X_test.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-preprocessing: Standardizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_tfidfw2v_vec = sc.fit_transform(X_train_tfidfw2v)\n",
    "X_test_tfidfw2v_vec = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8777642810079015\n",
      "0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_tfidfw2v_vec, y_1)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_tfidfw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8777642810079015\n",
      "0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                 \n",
    "random_search.fit(X_train_tfidfw2v_vec, y_1)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_tfidfw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8777357105487219\n",
      "0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
    "\n",
    "\n",
    "#Using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, scoring = 'accuracy', cv=3,n_jobs=-1,pre_dispatch=2)\n",
    "model.fit(X_train_tfidfw2v_vec, y_1)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(max(model.cv_results_['mean_train_score']))\n",
    "print(model.score(X_test_tfidfw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search with l1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.8777428532017822\n",
      "0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "param_dist = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}\n",
    "\n",
    "\n",
    "#Using Randomsearch cv\n",
    "random_search = RandomizedSearchCV(LogisticRegression(penalty='l1'),param_dist,scoring='accuracy',cv=3,n_jobs=-1,n_iter=5,pre_dispatch=2)\n",
    "                                   \n",
    "random_search.fit(X_train_tfidfw2v_vec, y_1)\n",
    "\n",
    "print(random_search.best_estimator_)\n",
    "print(max(random_search.cv_results_['mean_train_score']))\n",
    "print(random_search.score(X_test_tfidfw2v_vec, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l1 regularization with different value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.1, penalty='l1');\n",
    "clf.fit(X_train_tfidfw2v_vec, y_1);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.01, penalty='l1');\n",
    "clf.fit(X_train_tfidfw2v_vec, y_1);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.001, penalty='l1');\n",
    "clf.fit(X_train_tfidfw2v_vec, y_1);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=0.0005, penalty='l1');\n",
    "clf.fit(X_train_tfidfw2v_vec, y_1);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=10, penalty='l1');\n",
    "clf.fit(X_train_tfidfw2v_vec, y_1);\n",
    "w = clf.coef_\n",
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, if C decreases it's mean lambda increases and we can observe that if lambda increases then number of non zero elements decreases it's mean sparsity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table with model,lambda,train error and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------------+-------------+------------+\n",
      "|            Model            | Optimal lambda | Train error | Test error |\n",
      "+-----------------------------+----------------+-------------+------------+\n",
      "|     GRIDSEARCHCV_L2(BOW)    |     0.0001     |     0.06    |     9      |\n",
      "|    RANDOMSEARCHCV_L2(BOW)   |     0.001      |     0.06    |     9      |\n",
      "|     GRIDSEARCHCV_L1(BOW)    |      0.01      |     0.06    |     8      |\n",
      "|    RANDOMSEARCHCV_L1(BOW)   |      0.01      |     0.06    |     8      |\n",
      "|    GRIDSEARCHCV_L2(tfidf)   |     0.0001     |     0.01    |     8      |\n",
      "|   RANDOMSEARCHCV_L2(tfidf)  |     0.0001     |     0.01    |     8      |\n",
      "|    GRIDSEARCHCV_L1(tfidf)   |      0.01      |     0.01    |     8      |\n",
      "|   RANDOMSEARCHCV_L1(tfidf)  |      0.01      |     0.01    |     8      |\n",
      "|   GRIDSEARCHCV_L2(AVGW2V)   |     10000      |     0.11    |     15     |\n",
      "|  RANDOMSEARCHCV_L2(AVGW2V)  |     10000      |     0.11    |     15     |\n",
      "|   GRIDSEARCHCV_L1(AVGW2V)   |     10000      |     0.11    |     15     |\n",
      "|  RANDOMSEARCHCV_L1(AVGW2V)  |      100       |     0.11    |     15     |\n",
      "|  GRIDSEARCHCV_L2(TFIDFW2V)  |       1        |     0.13    |     17     |\n",
      "| RANDOMSEARCHCV_L2(TFIDFW2V) |       1        |     0.13    |     17     |\n",
      "|  GRIDSEARCHCV_L1(TFIDFW2V)  |      100       |     0.13    |     17     |\n",
      "| RANDOMSEARCHCV_L1(TFIDFW2V) |      100       |     0.13    |     17     |\n",
      "+-----------------------------+----------------+-------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Model\", \"Optimal lambda\", \"Train error\", \"Test error\"]\n",
    "\n",
    "x.add_row([\"GRIDSEARCHCV_L2(BOW)\", 0.0001,0.06,9])\n",
    "x.add_row([\"RANDOMSEARCHCV_L2(BOW)\", 0.001,0.06,9])\n",
    "x.add_row([\"GRIDSEARCHCV_L1(BOW)\", 0.01,0.06,8])\n",
    "x.add_row([\"RANDOMSEARCHCV_L1(BOW)\", 0.01,0.06,8])\n",
    "x.add_row([\"GRIDSEARCHCV_L2(tfidf)\", 0.0001,0.01,8])\n",
    "x.add_row([\"RANDOMSEARCHCV_L2(tfidf)\", 0.0001,0.01,8])\n",
    "x.add_row([\"GRIDSEARCHCV_L1(tfidf)\", 0.01,0.01,8])\n",
    "x.add_row([\"RANDOMSEARCHCV_L1(tfidf)\",  0.01,0.01,8])\n",
    "x.add_row([\"GRIDSEARCHCV_L2(AVGW2V)\", 10000,0.11,15])\n",
    "x.add_row([\"RANDOMSEARCHCV_L2(AVGW2V)\",10000,0.11,15])\n",
    "x.add_row([\"GRIDSEARCHCV_L1(AVGW2V)\", 10000,0.11,15])\n",
    "x.add_row([\"RANDOMSEARCHCV_L1(AVGW2V)\", 100,0.11,15])\n",
    "x.add_row([\"GRIDSEARCHCV_L2(TFIDFW2V)\", 1,0.13,17])\n",
    "x.add_row([\"RANDOMSEARCHCV_L2(TFIDFW2V)\", 1,0.13,17])\n",
    "x.add_row([\"GRIDSEARCHCV_L1(TFIDFW2V)\", 100,0.13,17])\n",
    "x.add_row([\"RANDOMSEARCHCV_L1(TFIDFW2V)\", 100,0.13,17])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's talk about the objective of this assignment first, In this assignment, we have to find optimal lambda with the help of gridsearchcv and random search cv with estimator logistic regression and we have the dataset which is amazon fine food reviews dataset which is high dimensional dataset. and also with l1 regularization and l2 regularization to find optimal lambda.\n",
    "2. Then we have to check the sparsity and the variation of sparsity mean number of non zero element after changing the value of optimal lambda and in this assignment we will also find the feature importance after checkng multicollinearity is there multicollinearity exist or not.Multicollinearity is a statistical phenomenon in which two or more predictor variables in a multiple logistic regression model are highly correlated or associated.\n",
    "3. Now talk about the solution of this assignment first of all we load the data as usual and preprocess the data then we will split the data in to train and test dataset then we will find bag of words with the help of vectorizer then i did the gridsearcv and randomsearchcv to find optimal lambda with l1 regularization and l2 regularization and i checked it for multicollinearity,after checking multicollinearity i find the most important features.\n",
    "4. Procedure to check the multicollinearity features, first of all i add the epsilon in our sparse matrix then i find the coefficient of both sparse matrix then we find the absolute difference of both the coefficient of sparse matrix if there will be the little bit difference then we can say there is no multicollinearity features exist in our matrix. \n",
    "5. Procedure to find the feature importance of our features, then i get the feature names of our sparse matrix and then i find the weight or coeeficient of our features then we sort the coefficient based on index of top most weight present in our data then we can get the top most or important features with the help of index.\n",
    "6. This all the procedure i did for bag of words,tfidf,avgw2v and tfidfw2v to find optimal lambda with the help of gridsearchcv and randomsearchcv with l1 and l2 regularization. \n",
    "7. we got the important or top most features for bag of words and tfidf. and we also checked the variation of sparsity after changing the value of optimal lambda. then we will see that if C decreases it's mean lambda increases and we can observe that if lambda increases then number of non zero elements decreases it's mean sparsity increases. we did it for all the technique to know how our sparse matrix change after changing the value of optimal lambda.\n",
    "8. And at last, I have draw a table with the help of library which is prettytable that contains all the information including with model, hyperparameter mean optimal lambda,train error and test error for each model that i have trained in our assignment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
